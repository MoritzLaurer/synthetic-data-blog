{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58113,"status":"ok","timestamp":1705772170944,"user":{"displayName":"Moritz Laurer","userId":"08772285247446402554"},"user_tz":-60},"id":"lvyHa31FD3Np","outputId":"d1184d0b-d1e6-434c-dd1a-a96cf32773dd"},"outputs":[],"source":["# install requirements\n","!pip install --upgrade pip -q\n","!pip install transformers~=4.37.2\n","!pip install huggingface_hub~=0.20.3\n","!pip install datasets~=2.16.1\n","!pip install openai~=1.11.0\n","!pip install scikit-learn\n","!pip install pandas\n","!pip install tqdm\n","!pip install python-dotenv\n","!pip install aiohttp\n","!pip install certifi "]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705772170944,"user":{"displayName":"Moritz Laurer","userId":"08772285247446402554"},"user_tz":-60},"id":"OcuBud1MtgQ9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Notebook running\n"]}],"source":["import os\n","from tqdm import tqdm\n","import ast\n","import numpy as np\n","import pandas as pd\n","import random\n","import json\n","from datetime import datetime\n","\n","from datasets import load_dataset\n","from datasets import concatenate_datasets\n","from openai import OpenAI, AsyncOpenAI\n","import openai\n","\n","import asyncio\n","from aiohttp import ClientSession, ClientTimeout, ClientError\n","from tqdm import tqdm\n","import random\n","import logging\n","\n","print(\"Notebook running\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/moritzlaurer/huggingface/projects/blog-posts/synthetic-data-blog\n"]}],"source":["# set correct directory\n","os.chdir(\"/Users/moritzlaurer/huggingface/projects/blog-posts/synthetic-data-blog/\")\n","print(os.getcwd())"]},{"cell_type":"markdown","metadata":{"id":"AGbwNfXdTSlq"},"source":["### Global variables"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# login via the huggingface hub with you hf_token\n","# you need a huggingface account and create a token here: https://huggingface.co/settings/tokens\n","# we can then call on the token with huggingface_hub.get_token()\n","import huggingface_hub\n","huggingface_hub.login()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1285,"status":"ok","timestamp":1705772172226,"user":{"displayName":"Moritz Laurer","userId":"08772285247446402554"},"user_tz":-60},"id":"lMeXv8BYs-Dg"},"outputs":[],"source":["# global variables for APIs\n","MODEL = \"Mixtral-8x7B-Instruct-v0.1\"  #\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\", \"Mixtral-8x7B-Instruct-v0.1\"\n","API_URL = f\"https://api-inference.huggingface.co/models/mistralai/{MODEL}\"\n","HEADERS = {\"Authorization\": f\"Bearer {huggingface_hub.get_token()}\"}\n","client_oai = AsyncOpenAI(api_key=os.getenv('OAI_TOKEN'))\n","\n","# choose one of these API providers: \"HF\" or \"OAI\"\n","API_PROVIDER = \"HF\"\n","# for asynchronous API calls\n","BATCH_SIZE = 64\n","SLEEP_TIME = 1\n","\n","# global variables for experiment variations\n","SEED = 42\n","N_SAMPLE = False  # You can sample parts of the data for faster testing. False for run on full dataset, int for sampling\n","SELF_CONSISTENCY_ITERATIONS = 3  # How many times should the model try to predict the same text for self-consistency?\n","DATA_SUBSET = \"sentences_allagree\"  # \"sentences_allagree\", \"sentences_66agree\", \"sentences_75agree\"\n","FINAL_TEST_RUN = True  # True for final run on test set\n","SAVE_OUTPUTS = True\n"]},{"cell_type":"markdown","metadata":{"id":"PNA50UZUSyZU"},"source":["### Load and prepare dataset"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['sentence', 'label', 'idx'],\n","    num_rows: 2264\n","})\n"]}],"source":["# financial_phrasebank paper: https://arxiv.org/pdf/1307.5336.pdf\n","random.seed(SEED)\n","\n","# load dataset\n","dataset = load_dataset(\n","    \"financial_phrasebank\", DATA_SUBSET, \n","    split=\"train\"  # note that the dataset does not have a default test split\n",")\n","\n","# sample for faster testing\n","if N_SAMPLE: \n","    dataset = dataset.select(random.sample(range(len(dataset)), N_SAMPLE))\n","\n","# train-test-split\n","# note: with 0-shot prompting you can sometimes skip holding out a test-set, because you don't train a model\n","# But: the prompt is a form of hyperparameter and every time to adapt the prompt \n","# to get better performance this is a form of hyperparameter search and you do not know how well this prompt would generalize to unseen data.\n","# If you update your prompt, it is therefore good practice to do the final test on a separate test-set\n","# on which the \"prompt wording hyperparameter\" was not tested to avoid overfitting your prompt to the data. \n","# Moreover: for our example, we need a separate testset because we will also train a small BERT model on the training data\n","\n","dataset = dataset.add_column(\"idx\", range(len(dataset)))\n","dataset = dataset.train_test_split(test_size=0.2, shuffle=True, stratify_by_column=\"label\", seed=SEED)\n","\n","# determine ids for train and test set to split again after inference\n","row_id_train = dataset[\"train\"][\"idx\"] \n","row_id_test = dataset[\"test\"][\"idx\"] \n","\n","if FINAL_TEST_RUN and (API_PROVIDER != \"OAI\"):\n","    # merging splits again here for easier inference. Splitting again after inference based on row_ids\n","    dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n","elif API_PROVIDER == \"OAI\":\n","    # for the run with OpenAI models, we only want labels for the testset to calculate metrics\n","    dataset = dataset[\"test\"]\n","else:\n","    # for testing prompts\n","    dataset = dataset[\"train\"]\n","\n","print(dataset)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4359f529954a40c3bdc9f0d13d6901dc","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2264 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['sentence', 'label', 'idx', 'label_text'],\n","    num_rows: 2264\n","})\n"]}],"source":["# create a new column with the numeric label verbalised as label_text (e.g. \"positive\" instead of \"0\")\n","label_map = {i: label_text for i, label_text in enumerate(dataset.features[\"label\"].names)}\n","\n","def add_label_text(example):\n","    example[\"label_text\"] = label_map[example[\"label\"]]\n","    return example\n","\n","dataset = dataset.map(add_label_text)\n","\n","print(dataset)"]},{"cell_type":"markdown","metadata":{"id":"xaX6xLzVS1C3"},"source":["### Prompts / Instructions"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705774111002,"user":{"displayName":"Moritz Laurer","userId":"08772285247446402554"},"user_tz":-60},"id":"8ILH8E7IG9wq"},"outputs":[],"source":["# prompt is inspired by the annotator instructions provided in section \"Annotation task and instructions\"\n","# in the financial_phrasebank paper: https://arxiv.org/pdf/1307.5336.pdf\n","\n","prompt_financial_sentiment = \"\"\"\\\n","You are a highly qualified expert trained to annotate machine learning training data.\n","\n","Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n","positive, negative, or neutral.\n","\n","Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n","\n","Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n","\n","Examples:\n","Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n","Label: positive\n","Text: The company generated net sales of 11.3 million euro this year.\n","Label: neutral\n","Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n","Label: negative\n","\n","Your TEXT to analyse:\n","TEXT: {text}\n","Label: \"\"\"\n","\n","\n","prompt_financial_sentiment_cot = \"\"\"\\\n","You are a highly qualified expert trained to annotate machine learning training data.\n","\n","Your task is to briefly analyze the sentiment in the TEXT below from an investor perspective and then label it with only one the three labels:\n","positive, negative, neutral.\n","\n","Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n","\n","You first reason step by step about the correct label and then return your label.\n","\n","You ALWAYS respond only in the following JSON format: {{\"reason\": \"...\", \"label\": \"...\"}}\n","You only respond with one single JSON response. \n","\n","Examples:\n","Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n","JSON response: {{\"reason\": \"An increase in operating profit is positive for investors\", \"label\": \"positive\"}}\n","Text: The company generated net sales of 11.3 million euro this year.\n","JSON response: {{\"reason\": \"The text only mentions financials without indication if they are better or worse than before\", \"label\": \"neutral\"}}\n","Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n","JSON response: {{\"reason\": \"A decrease in profit is negative for investors\", \"label\": \"negative\"}}\n","\n","Your TEXT to analyse:\n","TEXT: {text}\n","JSON response: \"\"\"\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"]}],"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n","\n","chat_financial_sentiment = [{\"role\": \"user\", \"content\": prompt_financial_sentiment}]\n","chat_financial_sentiment_cot = [{\"role\": \"user\", \"content\": prompt_financial_sentiment_cot}]\n","\n","prompt_financial_sentiment_formatted = tokenizer.apply_chat_template(chat_financial_sentiment, tokenize=False)\n","prompt_financial_sentiment_cot_formatted = tokenizer.apply_chat_template(chat_financial_sentiment_cot, tokenize=False)\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["'<s>[INST] You are a highly qualified expert trained to annotate machine learning training data.\\n\\nYour task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\\npositive, negative, or neutral.\\n\\nBase your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \\n\\nDo not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\\n\\nExamples:\\nText: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\\nLabel: positive\\nText: The company generated net sales of 11.3 million euro this year.\\nLabel: neutral\\nText: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\\t\\nLabel: negative\\n\\nYour TEXT to analyse:\\nTEXT: {text}\\nLabel:  [/INST]'"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["prompt_financial_sentiment_formatted"]},{"cell_type":"markdown","metadata":{},"source":["### Generation"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# params for API: https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\n","# alternative list for API: https://huggingface.github.io/text-generation-inference/#/Text%20Generation%20Inference/generate\n","# params for endpoints: https://huggingface.co/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient\n","\n","generation_params = dict(\n","    top_p=0.90,\n","    top_k=None,\n","    temperature=0.8,\n","    repetition_penalty=1.0,\n","    do_sample=True,\n","    max_new_tokens=128,\n","    return_full_text=False,\n","    #seed=SEED,  # no seed, because we need randomness for self-consistency\n","    max_time=None, \n","    stream=False,\n","    details=False,\n","    use_cache=False,\n","    wait_for_model=False,\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["\n","# avoid error: Request failed due to network error: Cannot connect to host api-inference.huggingface.co:443 ssl:True \n","# [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')]\n","# this might not be necessary on your machine\n","import ssl\n","import certifi\n","import aiohttp\n","ssl_context = ssl.create_default_context(cafile=certifi.where())"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# asynchronous functions for efficiently calling on LLM APIs with batching\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","\n","# functions for calling the HF API with retries, async and batch processing\n","async def request_with_retry_hf(session, url, headers, json, semaphore, retries=4, backoff_factor=3):\n","    \"\"\"Attempt a request with exponential backoff and retry logic.\"\"\"\n","    attempt = 0\n","    while attempt < retries:\n","        async with semaphore:\n","            try:\n","                async with session.post(url, headers=headers, json=json) as response:\n","                    if response.status in [200, 201]:\n","                        return await response.json()\n","                    elif response.status == 429:\n","                        retry_after = int(response.headers.get(\"Retry-After\", 60))\n","                        logging.warning(f\"Rate limit exceeded. Retrying after {retry_after} seconds.\")\n","                    else:\n","                        raise RuntimeError(f\"API returned a non-200 status code: {response.status}\")\n","            except (ClientError, asyncio.TimeoutError) as e:\n","                logging.error(f\"Request failed due to network error: {e}\")\n","            # Wait before retrying with exponential backoff\n","            sleep_time = backoff_factor ** attempt\n","            logging.info(f\"Retrying in {sleep_time} seconds...\")\n","            await asyncio.sleep(sleep_time)\n","            attempt += 1\n","    # After all retries, raise an exception to indicate the request has ultimately failed\n","    raise RuntimeError(\"Request failed after multiple retries.\")\n","\n","\n","async def generate_text_async_hf(session, text, prompt, generation_params, semaphore):\n","    payload = {\n","        \"inputs\": prompt.format(text=text),\n","        \"parameters\": {**generation_params}\n","    }\n","    # Call the request_with_retry function to handle potential retries\n","    response_json = await request_with_retry_hf(session, API_URL, HEADERS, payload, semaphore)\n","    generated_text = response_json[0].get(\"generated_text\", \"No text generated\")\n","    if \"error\" in response_json:\n","        raise RuntimeError(f\"API returned an error: {response_json['error']}\")\n","    return generated_text\n","\n","\n","async def request_with_retry_oai(session, messages, generation_params, semaphore, retries=4, backoff_factor=3):\n","    \"\"\"Attempt a request to the OpenAI API with exponential backoff and retry logic.\"\"\"\n","    attempt = 0\n","    while attempt < retries:\n","        async with semaphore:\n","            try:\n","                completion = await client_oai.chat.completions.create(\n","                    model=MODEL,\n","                    messages=messages,\n","                    **generation_params\n","                )\n","                return completion.choices[0].message.content\n","            except openai.RateLimitError as e:\n","                retry_after = int(e.headers.get(\"Retry-After\", 60))\n","                logging.warning(f\"Rate limit exceeded. Retrying after {retry_after} seconds.\")\n","                await asyncio.sleep(retry_after)\n","            except (openai.APITimeoutError, asyncio.TimeoutError) as e:\n","                logging.error(f\"Request failed due to API or network error: {e}\")\n","                sleep_time = backoff_factor ** attempt\n","                logging.info(f\"Retrying in {sleep_time} seconds...\")\n","                await asyncio.sleep(sleep_time)\n","                attempt += 1\n","    # After all retries, raise an exception to indicate the request has ultimately failed\n","    raise RuntimeError(\"Request failed after multiple retries.\")\n","\n","\n","async def generate_text_async_oai(session, text, prompt, generation_params, semaphore):\n","    messages = [{\"role\": \"user\", \"content\": prompt.format(text=text)}]\n","    if \"max_new_tokens\" in generation_params:\n","        generation_params[\"max_tokens\"] = generation_params.pop(\"max_new_tokens\")\n","    allowed_params = {\"top_p\", \"temperature\", \"max_tokens\", \"stop\"}\n","    generation_params = {k: v for k, v in generation_params.items() if k in allowed_params}\n","\n","    # Call the request_with_retry_oai function to handle potential retries\n","    generated_text = await request_with_retry_oai(session, messages, generation_params, semaphore)\n","    return generated_text\n","\n","\n","async def run_batch(dataset, prompt, generation_params, api_provider, batch_size, sleep_time):\n","    results_lst = []\n","    semaphore = asyncio.BoundedSemaphore(128)\n","    timeout = ClientTimeout(total=60)\n","\n","    #async with ClientSession(timeout=timeout) as session:\n","    async with ClientSession(timeout=timeout, connector=aiohttp.TCPConnector(ssl=ssl_context)) as session:\n","        for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing batches\"):\n","            text_batch = dataset[i:i + batch_size][\"sentence\"]\n","            if api_provider == \"HF\":\n","                tasks = [generate_text_async_hf(session, text, prompt, generation_params, semaphore) for text in text_batch]\n","            elif api_provider == \"OAI\":\n","                tasks = [generate_text_async_oai(session, text, prompt, generation_params, semaphore) for text in text_batch]\n","            else:\n","                raise ValueError(\"Invalid API provider\")\n","            results_batch = await asyncio.gather(*tasks)\n","            results_lst.extend(results_batch)\n","            await asyncio.sleep(sleep_time)\n","\n","    return results_lst\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing batches: 100%|██████████| 36/36 [02:43<00:00,  4.55s/it]"]},{"name":"stdout","output_type":"stream","text":["[' neutral', ' Neutral', ' neutral']\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# run batch processing for simple prompt\n","\n","# run async function in jupyter notebook\n","output_simple = await run_batch(dataset, prompt_financial_sentiment_formatted, generation_params, API_PROVIDER, BATCH_SIZE, SLEEP_TIME)\n","# run async function in .py script\n","#output_simple = asyncio.run(run_batch(dataset, prompt_financial_sentiment_formatted, generation_params, API_PROVIDER, BATCH_SIZE, SLEEP_TIME))\n","\n","print(output_simple[:3])"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing batches: 100%|██████████| 36/36 [04:39<00:00,  7.77s/it]\n","Processing batches: 100%|██████████| 36/36 [04:28<00:00,  7.46s/it]\n","Processing batches: 100%|██████████| 36/36 [04:21<00:00,  7.27s/it]"]},{"name":"stdout","output_type":"stream","text":["[' {\"reason\": \"The text mentions the company\\'s consideration of starting production in Russia, but it does not provide enough information to determine a clear sentiment from an investor\\'s perspective. The potential impact on investors would depend on various factors such as the cost-effectiveness, market potential, and risks associated with production in Russia.\", \"label\": \"neutral\"}', ' {\"reason\": \"The text does not contain any sentiment related to investments or financials, it is just a statement about the source of the text\", \"label\": \"neutral\"}', ' {\"reason\": \"The text does not provide information about the significance or impact of the deal, and does not reveal whether the deal is positive or negative for the company or its investors. Therefore, it is not possible to make a definitive sentiment judgment.\", \"label\": \"neutral\"}']\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# run batch processing for chain-of-thought and self-consistency prompt\n","output_cot_multiple = []\n","for _ in range(SELF_CONSISTENCY_ITERATIONS):\n","    # run async function in jupyter notebook\n","    output_cot = await run_batch(dataset, prompt_financial_sentiment_cot_formatted, generation_params, API_PROVIDER, BATCH_SIZE, SLEEP_TIME)\n","    # run async function in .py script\n","    #output_cot = asyncio.run(run_batch(dataset, prompt_financial_sentiment_cot_formatted, generation_params, API_PROVIDER, BATCH_SIZE, SLEEP_TIME))\n","    output_cot_multiple.append(output_cot)\n","\n","print(output_cot[:3])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# parse and clean outputs\n","random.seed(SEED)\n","\n","labels = [\"positive\", \"negative\", \"neutral\"]\n","\n","# function to map each label string to a discrete category (for simple prompt)\n","def clean_output(string, random_choice=True):\n","    for category in labels:\n","        if category.lower() in string.lower():\n","            return category\n","    if random_choice:\n","        return random.choice(labels)  # random category if no category is found\n","    else:\n","        return \"FAIL\"\n","\n","# function to parse chain-of-thought JSON output\n","def process_output_cot(output):\n","    try: \n","        output_dic = ast.literal_eval(output) \n","        return output_dic\n","    except Exception as e:\n","        # if json/dict parse fails, do simple search for occurance of first label term\n","        print(f\"Parsing failed for output: {output}, Error: {e}\")\n","        output_cl = clean_output(output, random_choice=True)\n","        output_dic = {\"reason\": \"FAIL\", \"label\": output_cl}\n","        return output_dic\n","\n","\n","# clean outputs for simple prompt\n","output_simple_cl = [clean_output(output) for output in output_simple]\n","\n","# clean outputs for CoT + SC prompt\n","output_cot_multiple_cl = []\n","output_dic_lst = []\n","for i in range(SELF_CONSISTENCY_ITERATIONS):\n","    output_dic_step = [process_output_cot(output) for output in output_cot_multiple[i]]\n","    output_labels = [dic[\"label\"] for dic in output_dic_step]\n","    output_cot_multiple_cl.append(output_labels)\n","    output_dic_lst.extend(output_dic_step)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# convert the CoT+SC output to dataframe for easier downstream processing\n","df_output = pd.DataFrame(data=output_cot_multiple_cl).T\n","df_output = df_output.rename(columns={0: \"sc_iter1\", 1: \"sc_iter2\", 2: \"sc_iter3\"})\n","df_output.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# find majority from multiple self-consistency runs\n","from collections import Counter\n","random.seed(SEED)\n","\n","def find_majority(row):\n","    # Find majority\n","    count = Counter(row)\n","    majority = count.most_common(1)[0]\n","    # Check if it's a real majority or if all 3 labels appear 3 times\n","    if majority[1] > 1:\n","        return majority[0]\n","    else: # in case all 3 labels appear 3 times\n","        return random.choice(labels)\n","\n","\n","# majority for multiple self-consistency + CoT runs\n","df_output['label_llm_cot_multiple'] = df_output.apply(find_majority, axis=1)\n","# single CoT run\n","df_output[\"label_llm_cot\"] = [clean_output(output, random_choice=True) for output in df_output[\"sc_iter1\"]]  # if parsing did not work, choose a random label\n","# simple labeling prompt\n","df_output[\"label_llm\"] = output_simple_cl\n","# expert label\n","df_output[\"label_experts\"] = dataset[\"label_text\"]\n","\n","df_output[\"text\"] = dataset[\"sentence\"]\n","\n","# also add the reasoning for the first iteration for inspection\n","df_output[\"reason_iter1\"] = [dic[\"reason\"] for dic in output_dic_lst[:len(df_output)]]\n","df_output[\"reason_iter2\"] = [dic[\"reason\"] for dic in output_dic_lst[len(df_output):len(df_output)*2]]\n","df_output[\"reason_iter3\"] = [dic[\"reason\"] for dic in output_dic_lst[len(df_output)*2:len(df_output)*3]]\n","\n","df_output.head(3)\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# save for debugging and in case something fails after API calls\n","df_output.to_csv(f'./data/df_{API_PROVIDER}_{MODEL}_backup.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["#### Calculate metrics"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, accuracy_score, classification_report\n","\n","def compute_metrics(label_experts, label_pred):\n","\n","    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(label_experts, label_pred, average='macro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n","    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(label_experts, label_pred, average='micro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n","    acc_balanced = balanced_accuracy_score(label_experts, label_pred)\n","    acc_not_balanced = accuracy_score(label_experts, label_pred)\n","\n","    metrics = {\n","        'f1_macro': f1_macro,\n","        'f1_micro': f1_micro,\n","        'accuracy_balanced': acc_balanced,\n","        'accuracy': acc_not_balanced,\n","        'precision_macro': precision_macro,\n","        'recall_macro': recall_macro,\n","        'precision_micro': precision_micro,\n","        'recall_micro': recall_micro,\n","    }\n","    metrics_report = classification_report(\n","        label_experts, label_pred, digits=2, output_dict=True, zero_division='warn'\n","    )\n","\n","    return {**metrics, **{\"report\": metrics_report}}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compute metrics\n","if FINAL_TEST_RUN:\n","    if \"HF\" in API_PROVIDER:\n","        df_output_test = df_output.iloc[row_id_test]\n","        df_output_train = df_output.iloc[row_id_train]\n","    elif API_PROVIDER == \"OAI\":\n","        df_output_test = df_output\n","        df_output_train = pd.DataFrame()\n","    else:\n","        raise NotImplementedError\n","    print(f\"Length of testset: {len(df_output_test)}, and length of trainset: {len(df_output_train)}\")\n","else:\n","    df_output_test = df_output.copy(deep=True)\n","    df_output_train = pd.DataFrame()\n","\n","\n","label_experts = df_output_test[\"label_experts\"]\n","label_llm = df_output_test[\"label_llm\"]\n","label_llm_cot = df_output_test[\"label_llm_cot\"]  #[label if label in labels else random.choice(labels) for label in df_output[\"label_llm_cot\"]]\n","label_llm_cot_multiple = df_output_test[\"label_llm_cot_multiple\"]  #[label if label in labels else random.choice(labels) for label in df_output[\"label_llm_cot_multiple\"]] # replacing FAIL with a random label\n","\n","metrics_single = compute_metrics(label_experts, label_llm)\n","metrics_single_cot = compute_metrics(label_experts, label_llm_cot)\n","metrics_multiple_cot =  compute_metrics(label_experts, label_llm_cot_multiple)\n","\n","metrics = {\"metrics_single\": metrics_single, \"metrics_single_cot\": metrics_single_cot, \"metrics_multiple_cot\": metrics_multiple_cot}\n","\n","metrics"]},{"cell_type":"markdown","metadata":{},"source":["### Save results"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["# save to disk\n","if SAVE_OUTPUTS: \n","    time_now = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n","\n","    file_path_metrics = f'./data/metrics_{API_PROVIDER}_{time_now}_{MODEL}.json'\n","\n","    # Writing the metrics dictiontary to json\n","    with open(file_path_metrics, 'w') as file:\n","        json.dump(metrics, file, indent=4)\n","\n","    # Write dfs to csv\n","    df_output_train.to_csv(f'./data/df_train_{API_PROVIDER}_{time_now}_{MODEL}.csv', index=False)\n","    df_output_test.to_csv(f'./data/df_test_{API_PROVIDER}_{time_now}_{MODEL}.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# inspect the texts where the model and the dataset labels from the experts disagree\n","df_wrong = df_output[df_output[\"label_llm_cot_multiple\"] != df_output[\"label_experts\"]]"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOcwY6KN9Dpy7bY0RrBnU7G","provenance":[]},"kernelspec":{"display_name":"Python (myenv)","language":"python","name":"myenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03f56d49d1d24b43b1bf0690c71f63ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dbbb13307ed42faad32e517c2311362":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19b0e2d3008a437ba69025380b8d44f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"294f43eb582544848ca58fffbb888e6c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c5746f523aa40c280fe50548f4199b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"336d81a811cb486e80197c3c4a522174":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3588920f173f43a9a31a453e5797843a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dbbb13307ed42faad32e517c2311362","placeholder":"​","style":"IPY_MODEL_bfecaf37ecce4f74ade13ebdaa042c64","value":"Resolving data files: 100%"}},"3c421b4bfd7a40fd9ecf77f3e87692cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7db6fe097c14a3e913e90733f1f9a7f","placeholder":"​","style":"IPY_MODEL_ff7bb0e11c19425abf8a4e43f9d02716","value":"Map: 100%"}},"65b6e4dab9574641b5e30179ea56907c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c421b4bfd7a40fd9ecf77f3e87692cb","IPY_MODEL_a4618b08797c4586adbeaf576633fdb3","IPY_MODEL_6724fd0d7cc64db993ee163089b50186"],"layout":"IPY_MODEL_294f43eb582544848ca58fffbb888e6c"}},"6724fd0d7cc64db993ee163089b50186":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_951094de028e46f4b9fa3a157b562963","placeholder":"​","style":"IPY_MODEL_03f56d49d1d24b43b1bf0690c71f63ee","value":" 988/988 [00:00&lt;00:00, 8437.37 examples/s]"}},"7ce31cecc2814678ab858046bdfa3702":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"951094de028e46f4b9fa3a157b562963":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ea9225228bc481c8ecf3fab227117f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3588920f173f43a9a31a453e5797843a","IPY_MODEL_d61c432471e048a7a57f631352a42af1","IPY_MODEL_d3bfcce0cef2461888e714ace975d149"],"layout":"IPY_MODEL_19b0e2d3008a437ba69025380b8d44f2"}},"a4618b08797c4586adbeaf576633fdb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_336d81a811cb486e80197c3c4a522174","max":988,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0c8ea12cb4641698a6ac36edbb742d2","value":988}},"bfecaf37ecce4f74ade13ebdaa042c64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0c8ea12cb4641698a6ac36edbb742d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3bfcce0cef2461888e714ace975d149":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddbe259f9fff42cb9251b582e624d3a9","placeholder":"​","style":"IPY_MODEL_7ce31cecc2814678ab858046bdfa3702","value":" 5534/5534 [00:02&lt;00:00, 4270.68it/s]"}},"d61c432471e048a7a57f631352a42af1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4a8592f5c9045e99367c532a88b05f2","max":5534,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c5746f523aa40c280fe50548f4199b1","value":5534}},"ddbe259f9fff42cb9251b582e624d3a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4a8592f5c9045e99367c532a88b05f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7db6fe097c14a3e913e90733f1f9a7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff7bb0e11c19425abf8a4e43f9d02716":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
